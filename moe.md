## Mixture of Experts and Expert Parallelism

# DeepEP
* https://github.com/deepseek-ai/DeepEP
* DeepEP Benchmark https://github.com/pbelevich/deepep-benchmark

# Perplexity Kernels
* https://github.com/ppl-ai/pplx-kernels
* Efficient and Portable Mixture-of-Experts Communication: https://www.perplexity.ai/hub/blog/efficient-and-portable-mixture-of-experts-communication
* Perplexity Kernels Benchmark: https://github.com/pbelevich/pplx-kernels-benchmark

# PyTorch Symmetric Memory
* [PyTorch Symmetric Memory: A New Paradigm for Programming Distributed AI](https://youtu.be/fX0tzsuGLUs)
  * https://github.com/pytorch/pytorch/blob/main/test/distributed/test_nvshmem.py
  * https://github.com/pytorch/pytorch/blob/main/test/distributed/test_nvshmem_triton.py
  * https://github.com/pytorch/pytorch/blob/main/test/distributed/test_symmetric_memory.py

# Triton Distributed
* https://github.com/ByteDance-Seed/Triton-distributed
* Paper: https://arxiv.org/pdf/2504.19442
